{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41b7dbbb-d2d2-475d-a9ba-146b0f1b5376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "#!pip install yfinance\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f47649a5-dc1b-41bf-be54-62a5ced3c1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72635459-0eca-4a74-a2b0-7f76bc5ae301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/28 04:05:41 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# SparkSession başlatma\n",
    "spark = SparkSession.builder.appName(\"NVDA Price Prediction\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cb55655-8673-4950-9282-cac36c8dce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veriyi yükleme ve DataFrame oluşturma\n",
    "data = spark.read.csv(\"file:///home/hduser/Desktop/NVDA/NVDA_histrical_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 'Date' sütununu tarih tipine dönüştürme \n",
    "data = data.withColumn(\"Date\", to_date(data[\"Date\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6f91e3d-3dd6-409e-8690-b9ca2aed1ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-------------------+-------------------+-------------------+--------+\n",
      "|      Date|               Open|               High|                Low|              Close|          Adj Close|  Volume|\n",
      "+----------+-------------------+-------------------+-------------------+-------------------+-------------------+--------+\n",
      "|1999-03-22| 0.4466150104999542|0.44791701436042786|0.42447900772094727|0.42447900772094727| 0.3893754184246063| 3667200|\n",
      "|1999-03-23|0.42708298563957214|0.42708298563957214|           0.390625| 0.3984380066394806|0.36548805236816406|16396800|\n",
      "|1999-03-24|0.39583298563957214| 0.3984380066394806|0.38020798563957214|0.39583298563957214|0.36309847235679626| 6086400|\n",
      "|1999-03-25| 0.3945310115814209|0.41666701436042786|0.39322900772094727|0.40104201436042786|0.36787667870521545| 4032000|\n",
      "|1999-03-26|            0.40625|             0.4375|            0.40625|0.43619799613952637| 0.4001253545284271| 8827200|\n",
      "+----------+-------------------+-------------------+-------------------+-------------------+-------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b60a8965-20f2-434d-af31-490396106316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+------------------+--------+\n",
      "|      Date|              Open|              High|               Low|             Close|         Adj Close|  Volume|\n",
      "+----------+------------------+------------------+------------------+------------------+------------------+--------+\n",
      "|2023-03-15|237.61000061035156|242.86000061035156|233.60000610351562|242.27999877929688|242.20230102539062|52448600|\n",
      "|2023-03-16|240.27000427246094| 255.8800048828125|238.94000244140625|255.41000366210938|255.32810974121094|58325300|\n",
      "|2023-03-17|259.82000732421875|  263.989990234375|256.67999267578125|            257.25|257.16754150390625|84854700|\n",
      "|2023-03-20| 256.1499938964844|  260.239990234375| 251.3000030517578|             259.0| 258.9169616699219|43274700|\n",
      "|2023-03-21|261.79998779296875| 263.9200134277344|253.80999755859375|  261.989990234375| 261.9059753417969|54740800|\n",
      "+----------+------------------+------------------+------------------+------------------+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(data.tail(5)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b95625e-da35-47e2-8ca4-b33e7bb74e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "|      Date|              Close|\n",
      "+----------+-------------------+\n",
      "|1999-03-22|0.42447900772094727|\n",
      "|1999-03-23| 0.3984380066394806|\n",
      "|1999-03-24|0.39583298563957214|\n",
      "|1999-03-25|0.40104201436042786|\n",
      "|1999-03-26|0.43619799613952637|\n",
      "+----------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 'Close' dışındaki tüm sütunları at\n",
    "df_org = data.select(\"Date\", \"Close\")\n",
    "\n",
    "# İlk 5 satırı göstermek için\n",
    "df_org.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ce644ac-6bc2-4c3d-8146-85ca41c02021",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5709ac83-64c7-4710-b4cc-c8562ee82ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Date|Close|\n",
      "+----+-----+\n",
      "|   0|    0|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "# Her bir sütun için eksik değer sayısını hesaplama\n",
    "missing_values = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "01f9b4a1-168c-4cf1-9248-defcc7d51409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Close' sütunu ve 'Date' sütunu ile sınırlı DataFrame'i CSV olarak kaydet\n",
    "#df.write.csv(\"file:///home/hduser/Desktop/NVDA/NVDA.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfea999c-1a6c-4627-82ac-14441b244c08",
   "metadata": {},
   "source": [
    "Uyarıları tamamen kapatmanın bir yolu olmasa da, günlükleri daha az ayrıntılı hale getirerek uyarıların gösterilmesini azaltabilirsiniz. Bu, Spark'ın günlük seviyesini ayarlayarak yapılabilir. Örneğin, günlük seviyesini \"ERROR\" olarak ayarlamak yalnızca hata mesajlarının gösterilmesini sağlayacaktır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f08dcf9a-ecc5-473c-b1c4-561c335c83e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c14ff527-6ae6-432b-9500-2f79ec30d2dd",
   "metadata": {},
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Stock Price Forecasting\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "407d4ec2-d6c5-472a-a59b-f31f6b6e8a3a",
   "metadata": {},
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, col, year\n",
    "\n",
    "# Tarih sütununa göre bölümlendirme yaparak window tanımınızı oluşturun\n",
    "windowSpec = Window.partitionBy(year(\"Date\")).orderBy(\"Date\")\n",
    "\n",
    "# Gecikmeli öznitelikleri DataFrame'e ekleyin\n",
    "for i in range(1, 6):\n",
    "    df = df.withColumn(f\"lag_{i}\", lag(col(\"Close\"), i).over(windowSpec))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b4bfd5-7f78-488d-a7a7-4e514201bfc1",
   "metadata": {},
   "source": [
    "1. Veri Setinin Yüklenmesi ve Öznitelik Oluşturma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe86df66-b5f9-4ca5-8e3a-325307f31d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lag,col, to_date, dayofweek\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.window import Window\n",
    "# Spark oturumunu başlatma\n",
    "spark = SparkSession.builder.appName(\"Stock Price Forecasting\").getOrCreate()\n",
    "\n",
    "# Gecikmeli öznitelikler ekleyin\n",
    "for i in range(1, 6):\n",
    "    df = df.withColumn(f\"lag_{i}\", lag(col(\"Close\"), i).over(Window.orderBy(\"Date\")))\n",
    "\n",
    "# Haftanın günü gibi tarih özniteliklerini ekleyin\n",
    "df = df.withColumn(\"DayOfWeek\", dayofweek(col(\"Date\")))\n",
    "\n",
    "# Eğitim için kullanılacak özniteliklerin listesini oluştur\n",
    "input_cols = [f\"lag_{i}\" for i in range(1, 6)] + ['DayOfWeek']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f4d3503-44fa-4372-a24e-e27c7f28602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "for i in range(1, 6):\n",
    "    # 'lag' sütunlarını mevcut 'Close' değeriyle doldurun\n",
    "    df = df.withColumn(f\"lag_{i}\", F.coalesce(df[f\"lag_{i}\"], df[\"Close\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfd1e297-eba0-4980-9675-476f10d0a7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31225bc6-fcb8-4fb0-8b15-0d451a0f4c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dayofweek\n",
    "# 'DayOfWeek' sütunu için eksik değerleri doldurun\n",
    "#df = df.withColumn('DayOfWeek', dayofweek(df['Date']))\n",
    "# 'DayOfWeek' sütunu için `null` değerleri kontrol edin ve doldurun\n",
    "#df = df.na.fill({'DayOfWeek': 0})\n",
    "\n",
    "# VectorAssembler'ı tekrar çalıştırın\n",
    "vectorAssembler = VectorAssembler(inputCols=input_cols, outputCol='features')\n",
    "df = vectorAssembler.transform(df)\n",
    "\n",
    "# Veri setini eğitim ve test olarak ayırma\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d6582a-5410-4d42-9186-0a8c1661ed24",
   "metadata": {},
   "source": [
    "2. Model Eğitimi ve Katsayıların Gözlemlenmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "738104f9-0b89-40d3-acb8-403e486dcbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 0.09939249361496093\n",
      "Coefficients:\n",
      "lag_1 coefficient: 0.9663176389375417\n",
      "lag_2 coefficient: 0.02625701686935515\n",
      "lag_3 coefficient: 0.05823763429523521\n",
      "lag_4 coefficient: -0.036756306638830706\n",
      "lag_5 coefficient: -0.014273953896684722\n",
      "lag_6 coefficient: -0.015020844606799031\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Modeli tanımla ve eğit\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Close\")\n",
    "model = lr.fit(train_df)\n",
    "\n",
    "# Eğitim ve test setleri üzerinde tahminler yap\n",
    "train_predictions = model.transform(train_df)\n",
    "test_predictions = model.transform(test_df)\n",
    "\n",
    "# Model katsayılarını ve intercept değerini yazdır\n",
    "print(\"Intercept:\", model.intercept)\n",
    "print(\"Coefficients:\")\n",
    "for i, coeff in enumerate(model.coefficients):\n",
    "    print(f\"lag_{i+1} coefficient: {coeff}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20089d6f-ea8a-4883-ae3d-1c6c935591a6",
   "metadata": {},
   "source": [
    "# Model katsayılarını ve intercept değerini görüntüleme\n",
    "coefficients = model.coefficients\n",
    "intercept = model.intercept\n",
    "\n",
    "print(f\"Intercept (β0): {intercept}\")\n",
    "print(\"Coefficients (β1, β2, ..., βn):\")\n",
    "for i, coeff in enumerate(coefficients):\n",
    "    print(f\"β{i+1}: {coeff}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "968fd699-4472-4e4b-9f8a-9282d5bb7ed8",
   "metadata": {},
   "source": [
    "df.printSchema()  # Sütun tiplerini kontrol etmek için şemayı yazdırın\n",
    "df.show(6)         # DataFrame'in ilk birkaç satırını görmek için"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a07d02-b458-4fc7-8730-cc0017e442e9",
   "metadata": {},
   "source": [
    "3. Model Değerlendirme (RMSE ve MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5994bbc-7e37-4aa2-a9b9-3c6d0074823b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 2.3269726507204274, Train MSE: 5.414801717200852\n",
      "Test RMSE: 2.0887951595650978, Test MSE: 4.363065218622583\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# RMSE ve MSE değerlendirici tanımla ve hesapla\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"Close\")\n",
    "\n",
    "train_rmse = evaluator.evaluate(train_predictions, {evaluator.metricName: \"rmse\"})\n",
    "test_rmse = evaluator.evaluate(test_predictions, {evaluator.metricName: \"rmse\"})\n",
    "train_mse = evaluator.evaluate(train_predictions, {evaluator.metricName: \"mse\"})\n",
    "test_mse = evaluator.evaluate(test_predictions, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "print(f\"Train RMSE: {train_rmse}, Train MSE: {train_mse}\")\n",
    "print(f\"Test RMSE: {test_rmse}, Test MSE: {test_mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c4d999-b186-44b3-ab2e-68c8e0a61087",
   "metadata": {},
   "source": [
    "4. Gelecekteki 5 Gün İçin Tahminlerin Yapılması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae22d31d-ca98-4f56-a8bc-1cf4d9a620d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Burada son bilinen verileri kullanarak gelecekteki 5 gün için tahminler yapacağız.\n",
    "# Kodun bu kısmı, son kapanış fiyatlarından yola çıkarak yeni öznitelikler oluşturur ve tahminleri yapar.\n",
    "\n",
    "# Modelin en son tahminlerinden ve bilinen değerlerden yeni gecikmeli öznitelikler yaratma\n",
    "last_row = train_df.orderBy(F.desc(\"Date\")).first()\n",
    "last_known_values = [last_row[\"Close\"]] + [last_row[f\"lag_{i}\"] for i in range(1, 5)]\n",
    "\n",
    "# Gelecekteki tarihler için tahminler yapma\n",
    "future_dates = [last_row[\"Date\"] + timedelta(days=i) for i in range(1, 6)]\n",
    "future_predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "705cfd25-86b6-4796-aec9-616881207b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75cbd52f-fad4-45a9-bb50-1b4916036212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.date(2023, 3, 22),\n",
       " datetime.date(2023, 3, 23),\n",
       " datetime.date(2023, 3, 24),\n",
       " datetime.date(2023, 3, 25),\n",
       " datetime.date(2023, 3, 26)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "future_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66af2322-d168-4194-ad4f-8a5f01aca166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Date: date, Close: double, lag_1: double, lag_2: double, lag_3: double, lag_4: double, lag_5: double, DayOfWeek: int, features: vector]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66cafb1d-18c3-4de3-b399-37d359e174b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      Date|   predicted_close|\n",
      "+----------+------------------+\n",
      "|2023-03-22| 258.2656071960368|\n",
      "|2023-03-23| 252.2940693102789|\n",
      "|2023-03-24|239.18109114269188|\n",
      "|2023-03-25|254.42228215756532|\n",
      "|2023-03-26|247.37945246220337|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit, array\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Modeli ve son bilinen verileri yükleyin\n",
    "#trained_model = # Eğitilmiş modelinizin yüklendiği yer\n",
    "#last_known_values = # Son bilinen lag ve close değerleri\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "# Tahmin yapmak istediğiniz gün sayısını belirleyin\n",
    "future_days = 5  # Örneğin, 5 gün sonrasını tahmin etmek istiyorsanız\n",
    "\n",
    "from pyspark.sql.functions import to_date, max\n",
    "# Veri setindeki en son tarihi al\n",
    "last_date_row = df.select(max(to_date(\"Date\"))).collect()\n",
    "current_date = last_date_row[0][0]\n",
    "#current_date = datetime.now()  # Mevcut tarihi kullanarak tahminler yapın\n",
    "\n",
    "# Gelecek tarihler için boş bir DataFrame oluşturun\n",
    "future_dates = [current_date + timedelta(days=i) for i in range(1, future_days + 1)]\n",
    "future_predictions = []\n",
    "\n",
    "\n",
    "# Örnek olarak, eğer modeliniz 5 gecikme özelliği kullanıyorsa (lag_1, lag_2, ... lag_5),\n",
    "# ve last_known_values son bilinen 'Close' değeri ve 5 gecikme özelliğini içeriyorsa:\n",
    "lag_features = ['lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5']\n",
    "\n",
    "# Bu değerleri kullanarak yeni özellikler vektörünü oluşturun\n",
    "#new_features = [last_known_values[i] for i in lag_features]\n",
    "\n",
    "# Döngü ile tahminleri yapın\n",
    "for future_date in future_dates:\n",
    "    # Yeni özellikler vektörünü oluşturun\n",
    "    new_features = last_known_values[-len(lag_features):] + [last_known_values[0]]\n",
    "    new_features_vector = Vectors.dense(new_features)\n",
    "    \n",
    "    # Yeni veri setini oluşturun ve tahmin yapın\n",
    "    df_to_predict = spark.createDataFrame([(future_date, new_features_vector)], [\"Date\", \"features\"])\n",
    "    prediction = model.transform(df_to_predict)\n",
    "    \n",
    "    # Tahmin edilen değeri alın ve sonuç listesine ekleyin\n",
    "    predicted_close = prediction.select(\"prediction\").collect()[0][\"prediction\"]\n",
    "    future_predictions.append((future_date, predicted_close))\n",
    "    \n",
    "    # Sonraki tahmin için 'lag' değerlerini güncelleyin\n",
    "    last_known_values = new_features[1:] + [predicted_close]\n",
    "\n",
    "# Tahminleri içeren DataFrame'i oluşturun\n",
    "predictions_df = spark.createDataFrame(future_predictions, [\"Date\", \"predicted_close\"])\n",
    "predictions_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "044deb72-4418-4bf8-abfc-041b5b904c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|      Date|             Close|\n",
      "+----------+------------------+\n",
      "|2023-03-15|242.27999877929688|\n",
      "|2023-03-16|255.41000366210938|\n",
      "|2023-03-17|            257.25|\n",
      "|2023-03-20|             259.0|\n",
      "|2023-03-21|  261.989990234375|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(df_org.tail(5)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce4d284-ad90-411d-b249-8fce3b51024a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275e961d-ef9f-4caa-bebe-542b72fc4e79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd5e42-9ff4-4485-a439-b32aa79df3b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "db3122b8-6768-4244-aa5c-e24b8b92fb4f",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Öznitelikler ve hedef değişkeni uygun formatta düzenleme\n",
    "X_train = train_df[['lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'DayOfWeek']]\n",
    "y_train = train_df['Close']\n",
    "X_test = test_df[['lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'DayOfWeek']]\n",
    "y_test = test_df['Close']\n",
    "\n",
    "# Modeli tanımla ve eğit\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Tahminler yap\n",
    "train_predictions = model.predict(X_train)\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Hata metriklerini hesapla\n",
    "train_rmse = mean_squared_error(y_train, train_predictions, squared=False)\n",
    "test_rmse = mean_squared_error(y_test, test_predictions, squared=False)\n",
    "\n",
    "print(f\"Train RMSE: {train_rmse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9b4aea-1e7d-4a18-8e89-a1168bc05ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a3090a-5809-4dbc-b402-a6ac92489da4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
