{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41b7dbbb-d2d2-475d-a9ba-146b0f1b5376",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "import pandas as pd\n",
    "#!pip install yfinance\n",
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f47649a5-dc1b-41bf-be54-62a5ced3c1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import to_date\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72635459-0eca-4a74-a2b0-7f76bc5ae301",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/28 02:16:27 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "# SparkSession başlatma\n",
    "spark = SparkSession.builder.appName(\"NVDA Price Prediction\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1cb55655-8673-4950-9282-cac36c8dce23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veriyi yükleme ve DataFrame oluşturma\n",
    "data = spark.read.csv(\"file:///home/hduser/Desktop/NVDA/NVDA_histrical_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# 'Date' sütununu tarih tipine dönüştürme \n",
    "data = data.withColumn(\"Date\", to_date(data[\"Date\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6f91e3d-3dd6-409e-8690-b9ca2aed1ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-------------------+-------------------+-------------------+--------+\n",
      "|      Date|               Open|               High|                Low|              Close|          Adj Close|  Volume|\n",
      "+----------+-------------------+-------------------+-------------------+-------------------+-------------------+--------+\n",
      "|1999-03-22| 0.4466150104999542|0.44791701436042786|0.42447900772094727|0.42447900772094727| 0.3893754184246063| 3667200|\n",
      "|1999-03-23|0.42708298563957214|0.42708298563957214|           0.390625| 0.3984380066394806|0.36548805236816406|16396800|\n",
      "|1999-03-24|0.39583298563957214| 0.3984380066394806|0.38020798563957214|0.39583298563957214|0.36309847235679626| 6086400|\n",
      "|1999-03-25| 0.3945310115814209|0.41666701436042786|0.39322900772094727|0.40104201436042786|0.36787667870521545| 4032000|\n",
      "|1999-03-26|            0.40625|             0.4375|            0.40625|0.43619799613952637| 0.4001253545284271| 8827200|\n",
      "+----------+-------------------+-------------------+-------------------+-------------------+-------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b60a8965-20f2-434d-af31-490396106316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+------------------+------------------+------------------+------------------+--------+\n",
      "|      Date|              Open|              High|               Low|             Close|         Adj Close|  Volume|\n",
      "+----------+------------------+------------------+------------------+------------------+------------------+--------+\n",
      "|2023-03-15|237.61000061035156|242.86000061035156|233.60000610351562|242.27999877929688|242.20230102539062|52448600|\n",
      "|2023-03-16|240.27000427246094| 255.8800048828125|238.94000244140625|255.41000366210938|255.32810974121094|58325300|\n",
      "|2023-03-17|259.82000732421875|  263.989990234375|256.67999267578125|            257.25|257.16754150390625|84854700|\n",
      "|2023-03-20| 256.1499938964844|  260.239990234375| 251.3000030517578|             259.0| 258.9169616699219|43274700|\n",
      "|2023-03-21|261.79998779296875| 263.9200134277344|253.80999755859375|  261.989990234375| 261.9059753417969|54740800|\n",
      "+----------+------------------+------------------+------------------+------------------+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.createDataFrame(data.tail(5)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1abc951d-c108-4ef6-bf74-4c91e57e0bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6138726e-2922-455a-b8db-f5e99602c3dc",
   "metadata": {},
   "source": [
    "# 'Close' dışındaki tüm sütunları at\n",
    "df = data.select(\"Date\", \"Close\")\n",
    "\n",
    "# İlk 5 satırı göstermek için\n",
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5709ac83-64c7-4710-b4cc-c8562ee82ace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+---+-----+---------+------+\n",
      "|Date|Open|High|Low|Close|Adj Close|Volume|\n",
      "+----+----+----+---+-----+---------+------+\n",
      "|   0|   0|   0|  0|    0|        0|     0|\n",
      "+----+----+----+---+-----+---------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, when\n",
    "\n",
    "# Her bir sütun için eksik değer sayısını hesaplama\n",
    "missing_values = df.select([count(when(col(c).isNull(), c)).alias(c) for c in df.columns])\n",
    "missing_values.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1820bd60-7cca-4b3a-8129-ea18b487bf92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01f9b4a1-168c-4cf1-9248-defcc7d51409",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'Close' sütunu ve 'Date' sütunu ile sınırlı DataFrame'i CSV olarak kaydet\n",
    "#df.write.csv(\"file:///home/hduser/Desktop/NVDA/NVDA.csv\", header=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfea999c-1a6c-4627-82ac-14441b244c08",
   "metadata": {},
   "source": [
    "Uyarıları tamamen kapatmanın bir yolu olmasa da, günlükleri daha az ayrıntılı hale getirerek uyarıların gösterilmesini azaltabilirsiniz. Bu, Spark'ın günlük seviyesini ayarlayarak yapılabilir. Örneğin, günlük seviyesini \"ERROR\" olarak ayarlamak yalnızca hata mesajlarının gösterilmesini sağlayacaktır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f08dcf9a-ecc5-473c-b1c4-561c335c83e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1778f1d7-9486-4c57-8a5a-63b09af3b831",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "c14ff527-6ae6-432b-9500-2f79ec30d2dd",
   "metadata": {},
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Stock Price Forecasting\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "407d4ec2-d6c5-472a-a59b-f31f6b6e8a3a",
   "metadata": {},
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, col, year\n",
    "\n",
    "# Tarih sütununa göre bölümlendirme yaparak window tanımınızı oluşturun\n",
    "windowSpec = Window.partitionBy(year(\"Date\")).orderBy(\"Date\")\n",
    "\n",
    "# Gecikmeli öznitelikleri DataFrame'e ekleyin\n",
    "for i in range(1, 6):\n",
    "    df = df.withColumn(f\"lag_{i}\", lag(col(\"Close\"), i).over(windowSpec))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b4bfd5-7f78-488d-a7a7-4e514201bfc1",
   "metadata": {},
   "source": [
    "1. Veri Setinin Yüklenmesi ve Öznitelik Oluşturma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe86df66-b5f9-4ca5-8e3a-325307f31d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lag,col, to_date, dayofweek\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.window import Window\n",
    "# Spark oturumunu başlatma\n",
    "spark = SparkSession.builder.appName(\"Stock Price Forecasting\").getOrCreate()\n",
    "\n",
    "# Gecikmeli öznitelikler ekleyin\n",
    "for i in range(1, 6):\n",
    "    df = df.withColumn(f\"lag_{i}\", lag(col(\"Close\"), i).over(Window.orderBy(\"Date\")))\n",
    "\n",
    "# Haftanın günü gibi tarih özniteliklerini ekleyin\n",
    "df = df.withColumn(\"DayOfWeek\", dayofweek(col(\"Date\")))\n",
    "\n",
    "# Eğitim için kullanılacak özniteliklerin listesini oluştur\n",
    "input_cols = [f\"lag_{i}\" for i in range(1, 6)] + ['Open', 'High', 'Low', 'Volume', 'DayOfWeek']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f4d3503-44fa-4372-a24e-e27c7f28602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "for i in range(1, 6):\n",
    "    # 'lag' sütunlarını mevcut 'Close' değeriyle doldurun\n",
    "    df = df.withColumn(f\"lag_{i}\", F.coalesce(df[f\"lag_{i}\"], df[\"Close\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cfd1e297-eba0-4980-9675-476f10d0a7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "31225bc6-fcb8-4fb0-8b15-0d451a0f4c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import dayofweek\n",
    "# 'DayOfWeek' sütunu için eksik değerleri doldurun\n",
    "#df = df.withColumn('DayOfWeek', dayofweek(df['Date']))\n",
    "# 'DayOfWeek' sütunu için `null` değerleri kontrol edin ve doldurun\n",
    "#df = df.na.fill({'DayOfWeek': 0})\n",
    "\n",
    "# VectorAssembler'ı tekrar çalıştırın\n",
    "vectorAssembler = VectorAssembler(inputCols=input_cols, outputCol='features')\n",
    "df = vectorAssembler.transform(df)\n",
    "\n",
    "# Veri setini eğitim ve test olarak ayırma\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d6582a-5410-4d42-9186-0a8c1661ed24",
   "metadata": {},
   "source": [
    "2. Model Eğitimi ve Katsayıların Gözlemlenmesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "738104f9-0b89-40d3-acb8-403e486dcbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept: 0.04888933354776447\n",
      "Coefficients:\n",
      "lag_1 coefficient: -0.07215812355408208\n",
      "lag_2 coefficient: -0.01639787411257297\n",
      "lag_3 coefficient: -0.006880200372128203\n",
      "lag_4 coefficient: 0.0033984379254951946\n",
      "lag_5 coefficient: 0.017200398278379554\n",
      "lag_6 coefficient: -0.5719389608544221\n",
      "lag_7 coefficient: 0.7894061625420268\n",
      "lag_8 coefficient: 0.859678723134829\n",
      "lag_9 coefficient: 9.72973647944246e-11\n",
      "lag_10 coefficient: -0.014206453313706149\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Modeli tanımla ve eğit\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Close\")\n",
    "model = lr.fit(train_df)\n",
    "\n",
    "# Eğitim ve test setleri üzerinde tahminler yap\n",
    "train_predictions = model.transform(train_df)\n",
    "test_predictions = model.transform(test_df)\n",
    "\n",
    "# Model katsayılarını ve intercept değerini yazdır\n",
    "print(\"Intercept:\", model.intercept)\n",
    "print(\"Coefficients:\")\n",
    "for i, coeff in enumerate(model.coefficients):\n",
    "    print(f\"lag_{i+1} coefficient: {coeff}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "20089d6f-ea8a-4883-ae3d-1c6c935591a6",
   "metadata": {},
   "source": [
    "# Model katsayılarını ve intercept değerini görüntüleme\n",
    "coefficients = model.coefficients\n",
    "intercept = model.intercept\n",
    "\n",
    "print(f\"Intercept (β0): {intercept}\")\n",
    "print(\"Coefficients (β1, β2, ..., βn):\")\n",
    "for i, coeff in enumerate(coefficients):\n",
    "    print(f\"β{i+1}: {coeff}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "968fd699-4472-4e4b-9f8a-9282d5bb7ed8",
   "metadata": {},
   "source": [
    "df.printSchema()  # Sütun tiplerini kontrol etmek için şemayı yazdırın\n",
    "df.show(6)         # DataFrame'in ilk birkaç satırını görmek için"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a07d02-b458-4fc7-8730-cc0017e442e9",
   "metadata": {},
   "source": [
    "3. Model Değerlendirme (RMSE ve MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5994bbc-7e37-4aa2-a9b9-3c6d0074823b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train RMSE: 0.8257366129326283, Train MSE: 0.6818409539374493\n",
      "Test RMSE: 0.8891719836317359, Test MSE: 0.7906268164755961\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# RMSE ve MSE değerlendirici tanımla ve hesapla\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"Close\")\n",
    "\n",
    "train_rmse = evaluator.evaluate(train_predictions, {evaluator.metricName: \"rmse\"})\n",
    "test_rmse = evaluator.evaluate(test_predictions, {evaluator.metricName: \"rmse\"})\n",
    "train_mse = evaluator.evaluate(train_predictions, {evaluator.metricName: \"mse\"})\n",
    "test_mse = evaluator.evaluate(test_predictions, {evaluator.metricName: \"mse\"})\n",
    "\n",
    "print(f\"Train RMSE: {train_rmse}, Train MSE: {train_mse}\")\n",
    "print(f\"Test RMSE: {test_rmse}, Test MSE: {test_mse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c4d999-b186-44b3-ab2e-68c8e0a61087",
   "metadata": {},
   "source": [
    "4. Gelecekteki 5 Gün İçin Tahminlerin Yapılması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae22d31d-ca98-4f56-a8bc-1cf4d9a620d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Burada son bilinen verileri kullanarak gelecekteki 5 gün için tahminler yapacağız.\n",
    "# Kodun bu kısmı, son kapanış fiyatlarından yola çıkarak yeni öznitelikler oluşturur ve tahminleri yapar.\n",
    "\n",
    "# Modelin en son tahminlerinden ve bilinen değerlerden yeni gecikmeli öznitelikler yaratma\n",
    "last_row = train_df.orderBy(F.desc(\"Date\")).first()\n",
    "last_known_values = [last_row[\"Close\"]] + [last_row[f\"lag_{i}\"] for i in range(1, 5)]\n",
    "\n",
    "# Gelecekteki tarihler için tahminler yapma\n",
    "future_dates = [last_row[\"Date\"] + timedelta(days=i) for i in range(1, 6)]\n",
    "future_predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "705cfd25-86b6-4796-aec9-616881207b62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------------+-------------------+-------------------+-------------------+--------+-------------------+-------------------+-------------------+-------------------+-------------------+---------+--------------------+\n",
      "|      Date|               Open|               High|                Low|              Close|          Adj Close|  Volume|              lag_1|              lag_2|              lag_3|              lag_4|              lag_5|DayOfWeek|            features|\n",
      "+----------+-------------------+-------------------+-------------------+-------------------+-------------------+--------+-------------------+-------------------+-------------------+-------------------+-------------------+---------+--------------------+\n",
      "|1999-03-22| 0.4466150104999542|0.44791701436042786|0.42447900772094727|0.42447900772094727| 0.3893754184246063| 3667200|0.42447900772094727|0.42447900772094727|0.42447900772094727|0.42447900772094727|0.42447900772094727|        2|[0.42447900772094...|\n",
      "|1999-03-23|0.42708298563957214|0.42708298563957214|           0.390625| 0.3984380066394806|0.36548805236816406|16396800|0.42447900772094727| 0.3984380066394806| 0.3984380066394806| 0.3984380066394806| 0.3984380066394806|        3|[0.42447900772094...|\n",
      "|1999-03-24|0.39583298563957214| 0.3984380066394806|0.38020798563957214|0.39583298563957214|0.36309847235679626| 6086400| 0.3984380066394806|0.42447900772094727|0.39583298563957214|0.39583298563957214|0.39583298563957214|        4|[0.39843800663948...|\n",
      "|1999-03-25| 0.3945310115814209|0.41666701436042786|0.39322900772094727|0.40104201436042786|0.36787667870521545| 4032000|0.39583298563957214| 0.3984380066394806|0.42447900772094727|0.40104201436042786|0.40104201436042786|        5|[0.39583298563957...|\n",
      "|1999-03-26|            0.40625|             0.4375|            0.40625|0.43619799613952637| 0.4001253545284271| 8827200|0.40104201436042786|0.39583298563957214| 0.3984380066394806|0.42447900772094727|0.43619799613952637|        6|[0.40104201436042...|\n",
      "+----------+-------------------+-------------------+-------------------+-------------------+-------------------+--------+-------------------+-------------------+-------------------+-------------------+-------------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cbd52f-fad4-45a9-bb50-1b4916036212",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d84d463-6c5e-4b2d-b508-dcd4c06e24e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = [f\"lag_{i}\" for i in range(1, 6)] + ['Open', 'High', 'Low', 'Volume', 'DayOfWeek']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b5737b-d1a9-4a00-b6b4-a4b60ae1d9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorAssembler = VectorAssembler(inputCols=input_cols, outputCol='features')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "443bdfce-5f82-4398-8449-d807b57b9140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Modeli tanımla ve eğit\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Close\")\n",
    "model = lr.fit(train_df)\n",
    "\n",
    "# Eğitim ve test setleri üzerinde tahminler yap\n",
    "train_predictions = model.transform(train_df)\n",
    "test_predictions = model.transform(test_df)\n",
    "\n",
    "# RMSE değerlendiricisini tanımla ve hesapla\n",
    "evaluator = RegressionEvaluator(predictionCol=\"prediction\", labelCol=\"Close\", metricName=\"rmse\")\n",
    "train_rmse = evaluator.evaluate(train_predictions)\n",
    "test_rmse = evaluator.evaluate(test_predictions)\n",
    "\n",
    "print(f\"Train RMSE: {train_rmse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfe8801-0416-44ed-9228-383fc5bbc8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "# İleri tarihli 5 gün için tahmin yapma\n",
    "last_row = train_df.orderBy(F.desc(\"Date\")).first()\n",
    "last_known_values = [last_row[\"Close\"]] + [last_row[f\"lag_{i}\"] for i in range(1, 5)]\n",
    "\n",
    "future_predictions = []\n",
    "\n",
    "for i in range(5):\n",
    "    # Yeni bir giriş özellik vektörü oluşturma\n",
    "    future_features = last_known_values[1:] + [last_known_values[0]]  # Özelliklerin sırası önemli olabilir\n",
    "    \n",
    "    # VectorAssembler kullanmadan giriş özelliklerini birleştirme\n",
    "    future_features_vector = spark.createDataFrame([(*future_features, last_row[\"DayOfWeek\"])], input_cols)\n",
    "    \n",
    "    # Modeli kullanarak tahmin yapma\n",
    "    prediction = model.transform(future_features_vector).collect()[0]['prediction']\n",
    "    \n",
    "    # Tahmini listeye ekleme\n",
    "    future_predictions.append(prediction)\n",
    "    \n",
    "    # Son bilinen değerleri güncelleme\n",
    "    last_known_values = [prediction] + last_known_values[:-1]\n",
    "\n",
    "    # Tarih bilgisini bir sonraki güne kaydırma\n",
    "    last_row[\"Date\"] += timedelta(days=1)\n",
    "\n",
    "# Tahminleri yazdırma\n",
    "for i, prediction in enumerate(future_predictions):\n",
    "    future_date = last_row[\"Date\"] + timedelta(days=i)\n",
    "    print(f\"Tarih: {future_date}, Tahmin: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e847ad02-6cfc-4ab6-b624-d9e34a856775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/28 02:16:52 ERROR Executor: Exception in task 31.0 in stage 36.0 (TID 86)\n",
      "org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (PredictionModel$$Lambda$3969/1820683712: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double).\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 5, y.size = 10\n",
      "\tat scala.Predef$.require(Predef.scala:281)\n",
      "\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:123)\n",
      "\tat org.apache.spark.ml.regression.LinearRegressionModel.predict(LinearRegression.scala:746)\n",
      "\tat org.apache.spark.ml.regression.LinearRegressionModel.predict(LinearRegression.scala:696)\n",
      "\tat org.apache.spark.ml.PredictionModel.$anonfun$transformImpl$1(Predictor.scala:204)\n",
      "\tat org.apache.spark.ml.PredictionModel.$anonfun$transformImpl$1$adapted(Predictor.scala:203)\n",
      "\t... 18 more\n",
      "24/03/28 02:16:52 ERROR TaskSetManager: Task 31 in stage 36.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o569.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 31 in stage 36.0 failed 1 times, most recent failure: Lost task 31.0 in stage 36.0 (TID 86) (192.168.125.213 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (PredictionModel$$Lambda$3969/1820683712: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 5, y.size = 10\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:123)\n\tat org.apache.spark.ml.regression.LinearRegressionModel.predict(LinearRegression.scala:746)\n\tat org.apache.spark.ml.regression.LinearRegressionModel.predict(LinearRegression.scala:696)\n\tat org.apache.spark.ml.PredictionModel.$anonfun$transformImpl$1(Predictor.scala:204)\n\tat org.apache.spark.ml.PredictionModel.$anonfun$transformImpl$1$adapted(Predictor.scala:203)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4036)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4206)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4204)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4204)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4033)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (PredictionModel$$Lambda$3969/1820683712: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 5, y.size = 10\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:123)\n\tat org.apache.spark.ml.regression.LinearRegressionModel.predict(LinearRegression.scala:746)\n\tat org.apache.spark.ml.regression.LinearRegressionModel.predict(LinearRegression.scala:696)\n\tat org.apache.spark.ml.PredictionModel.$anonfun$transformImpl$1(Predictor.scala:204)\n\tat org.apache.spark.ml.PredictionModel.$anonfun$transformImpl$1$adapted(Predictor.scala:203)\n\t... 18 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m df_future \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mcreateDataFrame([row])\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Modeli kullanarak tahmin yapma\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_future\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Tahmini listeye ekleme\u001b[39;00m\n\u001b[1;32m     22\u001b[0m future_predictions\u001b[38;5;241m.\u001b[39mappend(prediction)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:1216\u001b[0m, in \u001b[0;36mDataFrame.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns all the records as a list of :class:`Row`.\u001b[39;00m\n\u001b[1;32m   1197\u001b[0m \n\u001b[1;32m   1198\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;124;03m[Row(age=14, name='Tom'), Row(age=23, name='Alice'), Row(age=16, name='Bob')]\u001b[39;00m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sc):\n\u001b[0;32m-> 1216\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectToPython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, BatchedSerializer(CPickleSerializer())))\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o569.collectToPython.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 31 in stage 36.0 failed 1 times, most recent failure: Lost task 31.0 in stage 36.0 (TID 86) (192.168.125.213 executor driver): org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (PredictionModel$$Lambda$3969/1820683712: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 5, y.size = 10\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:123)\n\tat org.apache.spark.ml.regression.LinearRegressionModel.predict(LinearRegression.scala:746)\n\tat org.apache.spark.ml.regression.LinearRegressionModel.predict(LinearRegression.scala:696)\n\tat org.apache.spark.ml.PredictionModel.$anonfun$transformImpl$1(Predictor.scala:204)\n\tat org.apache.spark.ml.PredictionModel.$anonfun$transformImpl$1$adapted(Predictor.scala:203)\n\t... 18 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2785)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2721)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2720)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2720)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1206)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1206)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2984)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2923)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2912)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:971)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:448)\n\tat org.apache.spark.sql.Dataset.$anonfun$collectToPython$1(Dataset.scala:4036)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4206)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:526)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4204)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4204)\n\tat org.apache.spark.sql.Dataset.collectToPython(Dataset.scala:4033)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: [FAILED_EXECUTE_UDF] Failed to execute user defined function (PredictionModel$$Lambda$3969/1820683712: (struct<type:tinyint,size:int,indices:array<int>,values:array<double>>) => double).\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala:217)\n\tat org.apache.spark.sql.errors.QueryExecutionErrors.failedExecuteUserDefinedFunctionError(QueryExecutionErrors.scala)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:760)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:888)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:888)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: java.lang.IllegalArgumentException: requirement failed: BLAS.dot(x: Vector, y:Vector) was given Vectors with non-matching sizes: x.size = 5, y.size = 10\n\tat scala.Predef$.require(Predef.scala:281)\n\tat org.apache.spark.ml.linalg.BLAS$.dot(BLAS.scala:123)\n\tat org.apache.spark.ml.regression.LinearRegressionModel.predict(LinearRegression.scala:746)\n\tat org.apache.spark.ml.regression.LinearRegressionModel.predict(LinearRegression.scala:696)\n\tat org.apache.spark.ml.PredictionModel.$anonfun$transformImpl$1(Predictor.scala:204)\n\tat org.apache.spark.ml.PredictionModel.$anonfun$transformImpl$1$adapted(Predictor.scala:203)\n\t... 18 more\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Gelecekteki tarihler için tahminler yapma\n",
    "future_predictions = []\n",
    "\n",
    "for i in range(5):\n",
    "    # Yeni bir giriş özellik vektörü oluşturma\n",
    "    future_features = last_known_values[1:] + [last_known_values[0]]  # Özelliklerin sırası önemli olabilir\n",
    "    \n",
    "    # Özellikleri bir vektöre dönüştürme\n",
    "    features_vector = Vectors.dense(future_features)\n",
    "    \n",
    "    # Vektörü bir PySpark DataFrame içindeki vektör sütununa benzer bir şekilde işleme\n",
    "    row = Row(features=features_vector)\n",
    "    df_future = spark.createDataFrame([row])\n",
    "    \n",
    "    # Modeli kullanarak tahmin yapma\n",
    "    prediction = model.transform(df_future).select('prediction').collect()[0]['prediction']\n",
    "    \n",
    "    # Tahmini listeye ekleme\n",
    "    future_predictions.append(prediction)\n",
    "    \n",
    "    # Son bilinen değerleri güncelleme\n",
    "    last_known_values = [prediction] + last_known_values[:-1]\n",
    "\n",
    "    # Tarih bilgisini bir sonraki güne kaydırma\n",
    "    last_row[\"Date\"] += timedelta(days=1)\n",
    "\n",
    "# Tahminleri yazdırma\n",
    "for i, prediction in enumerate(future_predictions):\n",
    "    future_date = last_row[\"Date\"] + timedelta(days=i)\n",
    "    print(f\"Tarih: {future_date}, Tahmin: {prediction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cd5e42-9ff4-4485-a439-b32aa79df3b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb7ce41-91f6-48cd-8d89-42dd8d27a377",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25402ddb-c2e8-4995-b0d5-daf31182a9fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69688550-194e-49bb-9dca-0d27eaf8df31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4eac20-a8ed-4289-bd21-615ffb7d9b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4ad23a-6e68-4f29-bc94-57a03843adfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd517429-3b5e-4563-8741-f5e8646fdad8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d550f5e3-d086-4871-af80-bc1fa77c4888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036b6cea-6df0-42a3-b79a-a663d0f1af08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Öznitelikler ve hedef değişkeni uygun formatta düzenleme\n",
    "X_train = train_df[['lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'Open', 'High', 'Low', 'Volume', 'DayOfWeek']]\n",
    "y_train = train_df['Close']\n",
    "X_test = test_df[['lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5', 'Open', 'High', 'Low', 'Volume', 'DayOfWeek']]\n",
    "y_test = test_df['Close']\n",
    "\n",
    "# Modeli tanımla ve eğit\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Tahminler yap\n",
    "train_predictions = model.predict(X_train)\n",
    "test_predictions = model.predict(X_test)\n",
    "\n",
    "# Hata metriklerini hesapla\n",
    "train_rmse = mean_squared_error(y_train, train_predictions, squared=False)\n",
    "test_rmse = mean_squared_error(y_test, test_predictions, squared=False)\n",
    "\n",
    "print(f\"Train RMSE: {train_rmse}\")\n",
    "print(f\"Test RMSE: {test_rmse}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9b4aea-1e7d-4a18-8e89-a1168bc05ec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe932dbc-f786-460a-84d5-739cef730a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23981186-55f4-401a-8aad-c880509a3667",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a551714c-cfce-4714-8240-b4da6feed08d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fc1f03-0d69-461c-ab0a-fc8245318644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9daba5f-6272-4b74-a060-da19c5f00afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f755fb6c-daae-4625-93ef-da7cdf131a6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f7e735-794d-438c-b39d-ead3f8ef6cb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e675dcb-14a0-4d02-9bef-9de28280d019",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea68466-83b3-41bb-a6b6-51929cafb77a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3a8266-a20b-491a-bef5-3b4016504d3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b3b023-4a45-430a-933f-c107fd3b10f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22c0fc5-576b-44ba-83cc-a20ff82979ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3618b0c1-3407-4757-a522-a2b64507e0e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec2d712-ab74-4550-b1f1-99536e80732b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f10282-9de5-4a94-a1f2-ad8bf16b9c8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0474ebd-3676-4f53-94e3-4809c921cfba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9a3713-ea80-4edc-971d-e88d3e548150",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e65a216b-764b-401b-92ca-6b0b5e4cf9ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94ada5a-0576-4b46-a600-50d9e53cfc23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64af4d8e-727f-40de-90f7-4b21c40f9c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import lag,to_date, year, month, dayofmonth, dayofweek, dayofyear\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Spark session başlatma\n",
    "spark = SparkSession.builder.appName(\"Time Series Forecasting\").getOrCreate()\n",
    "\n",
    "# Veri setini yükleme ve tarih sütununu düzeltme\n",
    "df = df.withColumn(\"Date\", to_date(df[\"Date\"], \"yyyy-MM-dd\"))\n",
    "\n",
    "# Zaman damgası öznitelikleri ekleyin\n",
    "df = df.withColumn(\"Year\", year(\"Date\"))\n",
    "df = df.withColumn(\"Month\", month(\"Date\"))\n",
    "df = df.withColumn(\"DayOfMonth\", dayofmonth(\"Date\"))\n",
    "df = df.withColumn(\"DayOfWeek\", dayofweek(\"Date\"))\n",
    "df = df.withColumn(\"DayOfYear\", dayofyear(\"Date\"))\n",
    "\n",
    "# Gecikmeli öznitelikler oluştur\n",
    "windowSpec = Window.orderBy(\"Date\")\n",
    "for i in range(1, 6):\n",
    "    df = df.withColumn(f\"lag_{i}\", lag(\"Close\", i).over(windowSpec))\n",
    "\n",
    "# Null değerlerden kurtulun (ilk 5 satırda olabilir)\n",
    "df = df.na.drop()\n",
    "\n",
    "# Öznitelikleri bir vektör haline getirme\n",
    "feature_cols = [f\"lag_{i}\" for i in range(1, 6)] + [\"Year\", \"Month\", \"DayOfMonth\", \"DayOfWeek\", \"DayOfYear\"]\n",
    "vectorAssembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df = vectorAssembler.transform(df)\n",
    "\n",
    "# Veri setini eğitim ve test olarak ayırma\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Lineer regresyon modelini tanımlayın ve eğitin\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Close\")\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# Eğitim veri setinde tahminleri hesaplama\n",
    "train_predictions = lr_model.transform(train_df)\n",
    "test_predictions = lr_model.transform(test_df)\n",
    "\n",
    "# MSE ve RMSE değerlerini hesaplama\n",
    "evaluator = RegressionEvaluator(labelCol=\"Close\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "train_rmse = evaluator.evaluate(train_predictions)\n",
    "test_rmse = evaluator.evaluate(test_predictions)\n",
    "print(f\"Train RMSE: {train_rmse}, Test RMSE: {test_rmse}\")\n",
    "\n",
    "# Bu kısımda gelecekteki tarihler için tahminleri nasıl yapacağınıza bağlı olarak devam edebilirsiniz.\n",
    "# Örneğin, df'deki son tarih ve gerekli önceki kapanış fiyatları kullanılarak gelecekteki tarihler için tahmin yapılabilir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb0de80-cbd8-46de-bfd8-524a0643dd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model katsayılarını ve intercept değerini görüntüleme\n",
    "coefficients = model.coefficients\n",
    "intercept = model.intercept\n",
    "\n",
    "print(f\"Intercept (β0): {intercept}\")\n",
    "print(\"Coefficients (β1, β2, ..., βn):\")\n",
    "for i, coeff in enumerate(coefficients):\n",
    "    print(f\"β{i+1}: {coeff}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebf06f1-982c-46d6-8224-0fd2b1ac7d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Son bilinen kapanış fiyatlarını ve tarih bilgilerini al\n",
    "last_known_close_prices = df.select(\"Close\").orderBy(F.desc(\"Date\")).limit(5).collect()\n",
    "last_known_date = df.select(F.max(\"Date\")).collect()[0][0]\n",
    "\n",
    "# Gelecekteki tarihler için bir liste hazırlayın\n",
    "future_dates = [last_known_date + timedelta(days=i) for i in range(1, 6)]\n",
    "\n",
    "# Gelecekteki her bir tarih için tahminleri saklayacak bir liste\n",
    "future_predictions = []\n",
    "\n",
    "# Gelecekteki her bir tarih için tahmin yapmak için döngü\n",
    "for i in range(1, 6):\n",
    "    # Yeni bir DataFrame oluşturun\n",
    "    new_row = spark.createDataFrame([(future_dates[i-1],)], [\"Date\"])\n",
    "    \n",
    "    # Gelecekteki her bir tarih için gerekli gecikmeli öznitelikleri ekleyin\n",
    "    for j in range(5):\n",
    "        lag_col_name = f\"lag_{j+1}\"\n",
    "        if j < i:\n",
    "            lag_value = float(last_known_close_prices[j][0])\n",
    "        else:\n",
    "            # future_predictions listesine güvenli bir şekilde erişim\n",
    "            # İlk iterasyonda, listeye henüz tahmin eklenmemiş olacağından dolayı bir kontrol ekleniyor\n",
    "            lag_value = float(future_predictions[j-i][1]) if len(future_predictions) > j-i else None\n",
    "        \n",
    "        if lag_value is not None:\n",
    "            new_row = new_row.withColumn(lag_col_name, F.lit(lag_value))\n",
    "\n",
    "    # Öznitelik vektörünü oluşturun ve tahmin yapın\n",
    "    # Eğer lag_value None ise, bu satırda bir hata oluşacaktır, bu nedenle tüm lag değerleri doldurulmalıdır\n",
    "    new_row = vectorAssembler.transform(new_row)\n",
    "    prediction = lr_model.transform(new_row).select(\"prediction\").collect()[0][0]\n",
    "    \n",
    "    # Gelecekteki tahminleri listeye ekleyin\n",
    "    if None not in [lag_value for lag_value in lag_values]:\n",
    "        future_predictions.append((future_dates[i-1], prediction))\n",
    "        \n",
    "# Tahminleri DataFrame olarak dönüştürün ve gösterin\n",
    "future_predictions_df = spark.createDataFrame(future_predictions, [\"Date\", \"Prediction\"])\n",
    "future_predictions_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad90bdfa-acd9-4fdc-a309-5cff1643bc49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c3776f-1210-465d-bb37-1264cc506839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62759d95-2126-4e7e-aa79-d833f0bc83d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715bf0bd-466d-4b2d-814c-0286bb7d8426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f782793-38d9-4c12-8953-189c7e0c2f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dfcf55-56ad-41f5-956f-c2e4b56e23e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5be494b7-a0bb-4038-8cee-3624d42ed18d",
   "metadata": {},
   "source": [
    "1. Veri Setini Yükleme ve İlk İşleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e8ec9b-2ced-40e3-837c-857fe3af45a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# Spark session başlatma\n",
    "spark = SparkSession.builder.appName(\"Time Series Forecasting\").getOrCreate()\n",
    "\n",
    "# Veri setinin, 'Date' sütununu tarih tipine çevirme\n",
    "df = df.withColumn(\"Date\", to_date(df[\"Date\"], \"yyyy-MM-dd\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8cd512-b855-4028-a480-0b8715032041",
   "metadata": {},
   "source": [
    "2. Öznitelik Mühendisliği ve Veri Setini Bölme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58b2076-25fb-4b4c-ba41-b6dcca623ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lag\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Window tanımı yapma\n",
    "windowSpec = Window.orderBy(\"Date\")\n",
    "\n",
    "# Geçmiş kapanış fiyatlarından öznitelikler oluşturma (lag features)\n",
    "for i in range(1, 6):\n",
    "    df = df.withColumn(f\"lag_{i}\", lag(\"Close\", i).over(windowSpec))\n",
    "\n",
    "# Null değerleri kaldırma\n",
    "df = df.na.drop()\n",
    "\n",
    "# Öznitelikleri bir vektör haline getirme\n",
    "vectorAssembler = VectorAssembler(inputCols=[f\"lag_{i}\" for i in range(1, 6)], outputCol=\"features\")\n",
    "\n",
    "# Veri setini öznitelikler ve etiket (label) olarak ayarlama\n",
    "df = vectorAssembler.transform(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a0a0d8-132c-4fca-9f04-950ea306d326",
   "metadata": {},
   "source": [
    "3. Model Eğitimi ve Değerlendirme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc59300a-a2b5-4bcb-93c1-fe551ff838dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Modeli tanımlama ve eğitim veri seti üzerinde eğitme\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Close\")\n",
    "model = lr.fit(df)\n",
    "\n",
    "# Eğitim veri setindeki tahminleri hesaplama\n",
    "predictions = model.transform(df)\n",
    "\n",
    "# MSE ve RMSE değerlerini hesaplama\n",
    "evaluator = RegressionEvaluator(labelCol=\"Close\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "mse = evaluator.evaluate(predictions, {evaluator.metricName: \"mse\"})\n",
    "rmse = evaluator.evaluate(predictions, {evaluator.metricName: \"rmse\"})\n",
    "print(f\"MSE: {mse}, RMSE: {rmse}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81843ae-803c-43e9-96d5-dbed2206e13d",
   "metadata": {},
   "source": [
    "4. Geleceğe Yönelik Tahminlerin Yapılması\n",
    "\n",
    "Bu adım, mevcut veri setinizin sonundaki tarihler için 22-26 Mart tarihleri arasında 5 gün ekleyerek geleceğe yönelik tahminler yapılmasını içerir. Eğer bu tarihler veri setinizin dışındaysa, bu tahminleri yapabilmek için ekstra öznitelikler oluşturmanız gerekecektir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2050e50b-134e-439c-943c-657e0e57316b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "\n",
    "# En son tarihi al ve geleceğe dönük tarihler oluştur\n",
    "last_date = df.select(\"Date\").rdd.max()[0]\n",
    "future_dates = [last_date + timedelta(days=x) for x in range(1, 6)]\n",
    "future_df = pd.DataFrame(future_dates, columns=[\"Date\"])\n",
    "\n",
    "# Spark DataFrame'e dönüştür ve öznitelikler oluştur\n",
    "future_sdf = spark.createDataFrame(future_df)\n",
    "# Burada geleceğe yönelik öznitelikler oluşturulacak ve tahminler yapılacak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf85865-8a80-4cae-964c-98ea0e49d662",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_sdf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c209880e-63df-4e08-b440-46110deea55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9935f798-69de-40a2-ba9d-4cba5499c834",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Önceki adımlarda oluşturduğunuz modeli kullanarak tahmin yapma\n",
    "# 'future_sdf' içindeki gecikmeli özniteliklerin adları: 'lag_1', 'lag_2', 'lag_3', 'lag_4', 'lag_5'\n",
    "# Bu özniteliklerin nasıl hesaplanacağı modelinize ve veri setinize bağlıdır\n",
    "\n",
    "# Örneğin, son 5 günün kapanış fiyatlarını kullanarak bu öznitelikleri oluşturun:\n",
    "# Bu örnekte, 'df' mevcut veri setinizdir ve son günün kapanış fiyatlarına sahip olduğunuzu varsayıyoruz\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# 'future_sdf' için gerekli öznitelikleri oluşturma\n",
    "for i in range(1, 6):\n",
    "    # Son günün kapanış fiyatını 'lag_1' olarak kullanarak geriye doğru ilerleyin\n",
    "    # Burada veri setinizden uygun bir şekilde öznitelikleri doldurmanız gerekmekte\n",
    "    future_sdf = future_sdf.withColumn(f\"lag_{i}\", F.lit(df.select(F.col(\"Close\")).collect()[-i][0]))\n",
    "\n",
    "# Öznitelik vektörünü oluşturun\n",
    "future_sdf = vectorAssembler.transform(future_sdf)\n",
    "\n",
    "# Tahmin yapma\n",
    "future_predictions = model.transform(future_sdf)\n",
    "\n",
    "# Tahmin edilen değerleri gösterme\n",
    "future_predictions.select(\"Date\", \"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014fcf58-b0e9-440d-bf67-5ed078908a48",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# En son gerçek veriyi veya önceki tahmini kullanarak lag özniteliklerini güncelleme işlevi\n",
    "def update_lag_features(last_values, new_prediction):\n",
    "    # Son tahmin değerini ilk gecikme olarak ekleyin ve diğerlerini bir adım öteleme\n",
    "    updated_values = [new_prediction] + last_values[:-1]\n",
    "    return updated_values\n",
    "\n",
    "# İlk gecikme değerleri olarak mevcut bilinen son kapanış fiyatlarını kullanma\n",
    "last_known_values = [df.select(F.col(\"Close\")).collect()[-i][0] for i in range(1, 6)]\n",
    "\n",
    "# Gelecekteki her bir tarih için tahminler yapma\n",
    "for future_date in future_dates:\n",
    "    # Yeni lag özniteliklerini oluşturma\n",
    "    new_row = [future_date] + last_known_values\n",
    "    new_sdf = spark.createDataFrame([new_row], schema=future_sdf.schema)\n",
    "    \n",
    "    # Öznitelik vektörünü oluştur\n",
    "    new_sdf = vectorAssembler.transform(new_sdf)\n",
    "    \n",
    "    # Model ile tahmin yapma\n",
    "    new_prediction = model.transform(new_sdf).select(\"prediction\").collect()[0][0]\n",
    "    \n",
    "    # Tahminleri saklama ve sonraki adım için lag özniteliklerini güncelleme\n",
    "    last_known_values = update_lag_features(last_known_values, new_prediction)\n",
    "\n",
    "# Son tahminleri içeren DataFrame'i gösterme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b90a028-e713-40ce-8eec-242742c16adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# Gelecekteki her bir tarih için tahmin yapmak için döngü\n",
    "for i in range(1, 6):\n",
    "    # Yeni tahmin için gerekli gecikmeleri hazırla\n",
    "    # Burada son bilinen gerçek değerleri kullanıyorsunuz\n",
    "    lag_values = [float(df.select(F.col(\"Close\")).collect()[-j][0]) for j in range(i, i+5)]\n",
    "    future_date = last_date + timedelta(days=i)\n",
    "    \n",
    "    # Yeni bir satır oluştur. Features sütunu daha sonra doldurulacak\n",
    "    new_row = Row(Date=future_date, lag_1=lag_values[0], lag_2=lag_values[1], \n",
    "                  lag_3=lag_values[2], lag_4=lag_values[3], lag_5=lag_values[4])\n",
    "    \n",
    "    # Spark DataFrame'ine dönüştür ve öznitelik vektörünü oluştur\n",
    "    new_sdf = spark.createDataFrame([new_row])\n",
    "    new_sdf = vectorAssembler.transform(new_sdf)\n",
    "    \n",
    "    # Tahmin yap\n",
    "    new_prediction = model.transform(new_sdf).select(\"prediction\").collect()[0][0]\n",
    "    \n",
    "    # İleri tahminler için yeni gecikmeli değerler oluştur\n",
    "    lag_values.pop(0) # En eski lag değerini çıkar\n",
    "    lag_values.append(new_prediction) # Yeni tahmini ekle\n",
    "    \n",
    "    # Tahmini kaydet\n",
    "    future_predictions.append((future_date, new_prediction))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1929b0-e9f9-4e15-bcb0-ba168bdaab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gelecekteki tahminleri saklayacak bir liste oluştur\n",
    "future_predictions_list = []\n",
    "\n",
    "# Gelecekteki her bir tarih için tahmin yapmak için döngü\n",
    "for i in range(1, 6):\n",
    "    # Yeni tahmin için gerekli gecikmeleri hazırla\n",
    "    # Burada son bilinen gerçek değerleri kullanıyorsunuz\n",
    "    lag_values = [float(df.select(F.col(\"Close\")).collect()[-j][0]) for j in range(i, i+5)]\n",
    "    future_date = last_date + timedelta(days=i)\n",
    "    \n",
    "    # Yeni bir satır oluştur. Features sütunu daha sonra doldurulacak\n",
    "    new_row = Row(Date=future_date, lag_1=lag_values[0], lag_2=lag_values[1], \n",
    "                  lag_3=lag_values[2], lag_4=lag_values[3], lag_5=lag_values[4])\n",
    "    \n",
    "    # Spark DataFrame'ine dönüştür ve öznitelik vektörünü oluştur\n",
    "    new_sdf = spark.createDataFrame([new_row])\n",
    "    new_sdf = vectorAssembler.transform(new_sdf)\n",
    "    \n",
    "    # Tahmin yap\n",
    "    new_prediction = model.transform(new_sdf).select(\"prediction\").collect()[0][0]\n",
    "    \n",
    "    # İleri tahminler için yeni gecikmeli değerler oluştur\n",
    "    lag_values.pop(0) # En eski lag değerini çıkar\n",
    "    lag_values.append(new_prediction) # Yeni tahmini ekle\n",
    "    \n",
    "    # Tahmini ve tarihi listeye ekle\n",
    "    future_predictions_list.append((future_date, new_prediction))\n",
    "\n",
    "# Listeyi DataFrame'e dönüştür\n",
    "future_predictions_df = spark.createDataFrame(future_predictions_list, [\"Date\", \"Prediction\"])\n",
    "\n",
    "# Son tahminleri göster\n",
    "future_predictions_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c8a3d9-b401-4d0d-afc2-2d1a48e42ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f355d0-50c3-48b1-984b-51913b262d38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b42f89b-067d-4125-89fc-628e9ad26f5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8649d3-86f3-430e-bf78-79775efeb4d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344ea21c-96bc-43e0-891b-cbbc52d8a781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52e9593-e44b-4404-b96b-1033a1eef644",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b60d529-c097-445e-b666-fb621f625f61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18864f85-c0df-475a-9c12-8eb193c1e5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29580506-1cd5-4f78-a963-52302a0ff8d3",
   "metadata": {},
   "source": [
    "Adım 1: Öznitelik Mühendisliği\n",
    "Veri setinizi Spark DataFrame'e dönüştürün.\n",
    "lag fonksiyonunu kullanarak, her bir tarih için önceki günlerin kapanış fiyatlarını içeren yeni sütunlar ekleyin. Bu sütunlar, modelin öğrenme sürecinde kullanılacak öznitelikler olacak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b473a3-5a83-4618-8108-97170d066a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Time Series Forecasting\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a171156b-3c52-49b3-b315-d124051360ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag, col\n",
    "\n",
    "window = Window.orderBy(\"Date\")\n",
    "for i in range(1, 6):  # 5 günlük gecikmeler\n",
    "    df = df.withColumn(f\"lag_{i}\", lag(\"Close\", i).over(window))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb4a494-efe2-46ac-8317-98bcf56c5ea7",
   "metadata": {},
   "source": [
    "Adım 2: Model Eğitimi\n",
    "Öznitelikler hazır olduğunda, bir makine öğrenimi modeli seçin ve eğitin. Örneğin, RandomForestRegressor kullanabilirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d58b1f-0141-4ba3-89c5-22094f0e1c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[f\"lag_{i}\" for i in range(1, 6)], outputCol=\"features\")\n",
    "rf = RandomForestRegressor(featuresCol=\"features\", labelCol=\"Close\")\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# Eğitim veri setinde null değerler içeren satırlar kaldırılmalıdır\n",
    "df = df.na.drop()\n",
    "model = pipeline.fit(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a82539e-d03b-4ff9-9732-bc5292a5c00a",
   "metadata": {},
   "source": [
    "Adım 3: Tahmin\n",
    "Modeli eğittikten sonra, gelecek değerleri tahmin etmek için kullanabilirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73774071-316a-48ba-a785-d70173257b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.transform(df)\n",
    "predictions.select(\"Date\", \"Close\", \"prediction\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab5c89a-2eda-4ea8-81ab-ea29abc2a4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.createDataFrame(predictions.select(\"Date\", \"Close\", \"prediction\").tail(5)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12c8f48-bc7b-40d8-953c-8daea2957217",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a77bfd6-7409-4450-8939-0f5181fc735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Spark session başlatma\n",
    "spark = SparkSession.builder.appName(\"Future Dates\").getOrCreate()\n",
    "\n",
    "# Veri setinizin son tarihini string olarak alın\n",
    "last_date_str = '2023-03-21'\n",
    "last_date = datetime.strptime(last_date_str, \"%Y-%m-%d\")\n",
    "\n",
    "# Gelecekteki 5 gün için tarih listesi oluştur\n",
    "future_dates = [last_date + timedelta(days=x) for x in range(1, 6)]\n",
    "\n",
    "# Gelecekteki tarihler için pandas DataFrame oluştur\n",
    "future_df = pd.DataFrame(future_dates, columns=['Date'])\n",
    "\n",
    "# Pandas DataFrame'ini Spark DataFrame'ine dönüştür\n",
    "future_sdf = spark.createDataFrame(future_df)\n",
    "\n",
    "# Sonuçları göster\n",
    "future_sdf.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd53e074-16c9-4f78-a12f-ad6e2da51b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "# Öznitelikleri hazırlama ve vektör haline getirme\n",
    "vectorAssembler = VectorAssembler(inputCols=[\"lag_1\", \"lag_2\", \"lag_3\", \"lag_4\", \"lag_5\"], outputCol=\"features\")\n",
    "\n",
    "# Lineer regresyon modelini tanımlama\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"Close\")\n",
    "\n",
    "# Pipeline oluşturma\n",
    "pipeline = Pipeline(stages=[vectorAssembler, lr])\n",
    "\n",
    "# Modeli eğitme\n",
    "model = pipeline.fit(train_data)\n",
    "\n",
    "# Son 5 günlük tahminler için özniteliklerin oluşturulması\n",
    "# Bu kısım, mevcut verinizin son gününden itibaren ileri tarihler için yapılmalıdır\n",
    "# Örneğin, son gün 2023-03-21 ise, 2023-03-22 için öznitelikler oluşturup model.transform kullanarak tahmin yapmalısınız\n",
    "\n",
    "# Not: Bu örnekte, 'future_dates' adında ileri tarihleri içeren bir DataFrame'iniz olduğunu varsayıyoruz\n",
    "# Bu DataFrame, tahmin etmek istediğiniz tarihler için gerekli öznitelikleri (gecikmeleri vb.) içermelidir\n",
    "\n",
    "# Tahmin yapma\n",
    "predictions = model.transform(future_sdf)\n",
    "predictions.select(\"Date\", \"prediction\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699e8cc5-1f7d-41f0-bc1f-f2878a716fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b877a1-6c1e-400e-a0cd-1de3318c0863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe7616c-754b-4983-8473-fb696ed6456e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d56193-b971-4c00-b138-4cbf44a57c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceebed18-9476-463e-90c6-fa9fc6855ced",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb9e449-ece9-4ac1-b2da-d4d4a9696988",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c1e2f5-d739-4704-9f33-4988a5d1cb4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "b398c059-d971-42fc-92e8-9167fb874f3c",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import lit, percentile_approx\n",
    "\n",
    "# IQR - Interquartile Range hesaplayarak aykırı değerleri bulma\n",
    "bounds = {\n",
    "    c: dict(\n",
    "        zip([\"q1\", \"q3\"], df.approxQuantile(c, [0.25, 0.75], 0))\n",
    "    )\n",
    "    for c in ['Close'] # Aykırı değer kontrolü yapılacak sütunlar\n",
    "}\n",
    "\n",
    "for c in bounds:\n",
    "    iqr = bounds[c]['q3'] - bounds[c]['q1']\n",
    "    bounds[c]['min'] = bounds[c]['q1'] - (1.5 * iqr)\n",
    "    bounds[c]['max'] = bounds[c]['q3'] + (1.5 * iqr)\n",
    "\n",
    "# Aykırı değerleri filtreleme\n",
    "outliers = df.select(\n",
    "    '*',\n",
    "    *[\n",
    "        when(\n",
    "            (col(c) < bounds[c]['min']) | (col(c) > bounds[c]['max']),\n",
    "            c\n",
    "        ).alias(c + '_outlier') for c in bounds\n",
    "    ]\n",
    ")\n",
    "outliers.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0dc0c4-1e41-4ba1-bb15-d04b51bca0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d469b3-e2c9-4a70-8b79-432c82b3f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Özellikleri bir vektör haline getirme\n",
    "assembler = VectorAssembler(inputCols=['Open', 'High', 'Low', 'Volume'], outputCol=\"features\")\n",
    "df_assembled = assembler.transform(df)\n",
    "\n",
    "# Ölçeklendirme için StandardScaler kullanma\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=True)\n",
    "scalerModel = scaler.fit(df_assembled)\n",
    "df_scaled = scalerModel.transform(df_assembled)\n",
    "\n",
    "df_scaled.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3733a357-5e10-43fb-b037-da6375a21c84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf173c40-10df-48fe-aa43-f622ad264f33",
   "metadata": {},
   "source": [
    "# ozellik muhendisligi"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd15df1a-b9fb-4f39-8bdd-b78070dffa7c",
   "metadata": {},
   "source": [
    "# bir gunde birden fazla satir varsa kullanilir. mesela farkli borsalardan alinmis bir hisse senede gibi\n",
    "\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Sütunlarınızın isimlerine göre uyarlamanız gerekebilir.\n",
    "windowSpec = Window.partitionBy('Date').orderBy('Date').rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Örneğin, her bir 'GroupingColumn' grubu için 'TargetColumn' üzerinde hareketli ortalama hesaplayalım.\n",
    "df = df.withColumn('MovingAverage', avg('Close').over(windowSpec))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15082aa1-6d33-4091-92f3-ef6b41581c67",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "\n",
    "df = df.withColumn('Year', year(df['Date']))\n",
    "df = df.withColumn('Month', month(df['Date']))\n",
    "df = df.withColumn('Day', dayofmonth(df['Date']))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5081cf-aa43-4ee0-89b3-dc112e04800b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lag\n",
    "\n",
    "windowSpec = Window.orderBy('Date')\n",
    "df = df.withColumn('Prev_Close', lag(df['Close']).over(windowSpec))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e314dae-449f-404f-b386-bb29b433ecf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "\n",
    "short_window = Window.orderBy('Date').rowsBetween(-30, 0)\n",
    "long_window = Window.orderBy('Date').rowsBetween(-90, 0)\n",
    "\n",
    "df = df.withColumn('Short_Average', avg('Close').over(short_window))\n",
    "df = df.withColumn('Long_Average', avg('Close').over(long_window))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b592fc5a-7f4a-4a59-b54f-d3fdb9c60c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn('Day_Pct_Change', (col('Close') - col('Prev_Close')) / col('Prev_Close'))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31ddf4b-5550-4712-96d3-bbedcc02b6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import log\n",
    "\n",
    "df = df.withColumn('Log_Volume', log(col('Volume')))\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbf34d3-1605-49bb-8090-ad6367611f8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359aee9c-bcd7-46ed-ab28-ac0a180a6e86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008b7b5d-b0f5-4cff-bca5-888b77f7d781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220a58ae-044a-44ce-8b94-c819f7dde3be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf3d7a6-5ce1-43ef-ae56-739bf2407c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19553181-3613-4f53-81df-ae5f90dd9b91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59c7885-d629-4795-968a-b094ef969e63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5234bf-3ad2-445b-afd8-9a8e328e8ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specifying properties and target variable\n",
    "feature_columns = ['Open', 'High', 'Low', 'Volume']  # Örnek özellikler\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "data = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10bc78ca-3ebb-49d4-9ed9-4b9f8d66e6da",
   "metadata": {},
   "source": [
    "In this cell, we have selected the features that your machine learning model will use as input and turned them into a vector. The columns 'Open', 'High', 'Low', and 'Volume' are the features that will be used for the model to learn. Using VectorAssembler, we combine these features into a single vector column (in the 'features' column). This is done to meet the requirements of Spark's machine learning library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c252d9-ad38-4435-b525-3d58cd46c556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable\n",
    "data = data.withColumn(\"label\", data[\"Close\"])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e57d807a-285d-4eb5-9d7b-3d33eade8c42",
   "metadata": {},
   "source": [
    "In this cell, we have set the target variable that your model will try to predict. In this case, we have chosen the 'Close' column, i.e. the closing price of the stock, as the target and added it to the DataFrame as the 'label' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e49b86-f9dd-49b8-9310-eb5321c6bb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate training and test sets\n",
    "train_data, test_data = data.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc52e458-3d84-4364-97ef-74b93958cc77",
   "metadata": {},
   "source": [
    "The dataset is split into two parts for training and testing the model. Usually a large part of the dataset (80% in this case) is allocated for training the model and a small part (20% in this case) is allocated for testing the model. The seed parameter ensures that this split is the same each time the model is run, so that the results can be repeated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e68ccf-376c-4c50-8be9-3f331669f109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model creation and training\n",
    "lr = LinearRegression(featuresCol='features', labelCol='label')\n",
    "model = lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b984945-e44f-42b6-9c77-9ba48dbef67c",
   "metadata": {},
   "source": [
    "In this cell, we created a LinearRegression model and trained it using the training data. After the model was created, we fit it to the training data with the fit method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08337b1-f21b-4884-8a08-297b463feec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on the test set\n",
    "predictions = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e279fcc-0bc5-4b0b-9434-8e39349b1803",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6798f2d7-0368-4f74-8886-95daec739a28",
   "metadata": {},
   "source": [
    "With the transform method, we generated a prediction for each row in the test set and added these predictions to the test DataFrame as the 'prediction' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363f57b4-61da-413b-881f-8b0c87524ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating performance\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "print(f\"Root Mean Squared Error (RMSE) on test data: {rmse}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bcc978bb-4cca-4a90-a669-6840a4a3cee1",
   "metadata": {},
   "source": [
    "The performance of the model is evaluated using the difference between the actual values and the model's predictions. In this study, we calculate the root mean square error (RMSE) using the RegressionEvaluator class. RMSE is a measure of how accurate the predictions are; the lower the value, the better the model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c73dd17-9673-46fb-ad0e-501741d39a8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bde9ca-9f67-4124-91e5-539950345be5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
